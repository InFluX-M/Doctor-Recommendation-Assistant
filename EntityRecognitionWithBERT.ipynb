{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d7d521e-c6c0-4979-b855-b2d1f367a091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asusl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\asusl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\asusl\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.2)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers seqeval[gpu] pandas numpy sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46785b6",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4731ac97-9958-4832-a36e-85f1d7028d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification, PreTrainedTokenizer\n",
    "import mlflow\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a09b1f3-696a-49ca-999e-14d6d2f8121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2231f1",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "399d12cb-e530-4686-93c7-4d99d06e2bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>یک دکتر مرد خوب برای ارتوپد در کرمانشاه سراغ د...</td>\n",
       "      <td>O,O,B-gen,O,O,O,O,B-cit,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آیا می توانید یک دکتر برای آسم معرفی کنید ؟</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کسی یک دکتر مرد برای افسردگی در خرم آباد یا مش...</td>\n",
       "      <td>O,O,O,B-gen,O,O,O,B-cit,I-cit,O,B-cit,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آیا می توانید یک دکتر برای روانپزشکی معرفی کنید ؟</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آیا می توانید یک دکتر برای زنان معرفی کنید ؟</td>\n",
       "      <td>O,O,O,O,O,O,B-gen,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  یک دکتر مرد خوب برای ارتوپد در کرمانشاه سراغ د...   \n",
       "1        آیا می توانید یک دکتر برای آسم معرفی کنید ؟   \n",
       "2  کسی یک دکتر مرد برای افسردگی در خرم آباد یا مش...   \n",
       "3  آیا می توانید یک دکتر برای روانپزشکی معرفی کنید ؟   \n",
       "4       آیا می توانید یک دکتر برای زنان معرفی کنید ؟   \n",
       "\n",
       "                                         label  \n",
       "0                O,O,B-gen,O,O,O,O,B-cit,O,O,O  \n",
       "1                          O,O,O,O,O,O,O,O,O,O  \n",
       "2  O,O,O,B-gen,O,O,O,B-cit,I-cit,O,B-cit,O,O,O  \n",
       "3                          O,O,O,O,O,O,O,O,O,O  \n",
       "4                      O,O,O,O,O,O,B-gen,O,O,O  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/labeled_doctor_req_chatgpt.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14b01165-6ace-4509-a11e-9fa923b9e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-gen': 1,\n",
    "    'I-gen': 2,\n",
    "    'B-cit': 3,\n",
    "    'I-cit': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1460dde-3480-4687-81a3-adeafd8a7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'O',\n",
    "    1: 'B-gen',\n",
    "    2: 'I-gen',\n",
    "    3: 'B-cit',\n",
    "    4: 'I-cit'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbc397",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49a951c1-e578-4f20-8f43-067e775762e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asusl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dee005",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40d77460-803e-4539-a840-70c1026e6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence: str, text_labels: str, tokenizer: PreTrainedTokenizer) -> tuple[list[str], list[str]]:\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75bbdf06-635f-48b9-8d6a-84bc06cad836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer: PreTrainedTokenizer, max_len: int) -> None:\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index :int):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.label[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19af0c",
   "metadata": {},
   "source": [
    "## Split into Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bf055f2-291b-40b4-8e61-06fb051f8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1000, 2)\n",
      "TRAIN Dataset: (800, 2)\n",
      "TEST Dataset: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb59597c-76a1-4b72-a1ff-599655c27da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([    2,  3777,  4283,  3949,  2831,  7169,  2793, 59086,  1012,     4,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35e9f58b-549e-41cf-8019-a32cef969cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       O\n",
      "دنبال       O\n",
      "دکتر        O\n",
      "خوبی        O\n",
      "برای        O\n",
      "افسردگی     O\n",
      "می          O\n",
      "گردم        O\n",
      ".           O\n",
      "[SEP]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n",
      "[PAD]       O\n"
     ]
    }
   ],
   "source": [
    "# print the first 30 tokens and corresponding labels\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
    "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61c58c8e-6486-4128-90fc-1fa884dbb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9684ef",
   "metadata": {},
   "source": [
    "# Define and track models with mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1c5a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec69a91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/Learning/Hammasir Camp/Project/mlflow/900433234344106285', creation_time=1724746678302, experiment_id='900433234344106285', last_update_time=1724746678302, lifecycle_stage='active', name='NER', tags={}>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"mlflow\")\n",
    "mlflow.set_experiment(\"NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169509bb",
   "metadata": {},
   "source": [
    "## Writing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eb312b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(id2label: dict, label2id: dict, training_set: dataset) -> tuple[BertForTokenClassification, torch.Tensor]:\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        'HooshvareLab/bert-fa-base-uncased', \n",
    "        num_labels=len(id2label),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model.to(device)\n",
    "    ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "    mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "    targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "    ids = ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "    initial_loss = outputs[0]\n",
    "    return model, initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bd72ebb-5449-4f67-bb07-c24423b5fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(optimizer: torch.optim.Adam, max_norm: int, training_loader: DataLoader, model: BertForTokenClassification) -> tuple[BertForTokenClassification, float, float]:\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_norm\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    return model, epoch_loss, tr_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "432c9934-d6d3-453e-a6b9-2152d9cf1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model: BertForTokenClassification, testing_loader: DataLoader, device: str, id2label: dict, label2id: dict) -> tuple[list[str], list[str], float, float]:\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "    return labels, predictions, eval_loss, eval_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0027504-f122-4884-a07e-1191ff34ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence: str, model: BertForTokenClassification, tokenizer: BertTokenizer, id2label: dict, device: str) -> tuple[str, list[str]]:\n",
    "  inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "  # move to gpu\n",
    "  ids = inputs[\"input_ids\"].to(device)\n",
    "  mask = inputs[\"attention_mask\"].to(device)\n",
    "  # forward pass\n",
    "  outputs = model(ids, mask)\n",
    "  logits = outputs[0]\n",
    "\n",
    "  active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "  tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "  token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "  word_level_predictions = []\n",
    "  for pair in wp_preds:\n",
    "    if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "      # skip prediction\n",
    "      continue\n",
    "    else:\n",
    "      word_level_predictions.append(pair[1])\n",
    "\n",
    "  # we join tokens, if they are not special ones\n",
    "  str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "  return str_rep, word_level_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c97b8b",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0eb3366-f0e4-416e-a972-29de1c5fbdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5820e94210344947ad5f30fbd68a8633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   2%|1         | 10.5M/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 100 training steps: 1.3812986612319946\n",
      "Training loss per 100 training steps: 0.08356071243726529\n"
     ]
    }
   ],
   "source": [
    "mlflow.transformers.autolog(disable=True)\n",
    "with mlflow.start_run(run_name='base_line_city_gender'):\n",
    "    mlflow.log_params({\n",
    "        'EPOCHS': EPOCHS,\n",
    "        'LEARNING_RATE': LEARNING_RATE,\n",
    "        'MAX_GRAD_NORM': MAX_GRAD_NORM\n",
    "    })\n",
    "    model, initial_loss = initialize_model(label2id=label2id, id2label=id2label, training_set=training_set)\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "    for i in range(EPOCHS):\n",
    "        model, epoch_loss, tr_accuracy = train(optimizer, MAX_GRAD_NORM, training_loader, model)\n",
    "        mlflow.log_metric(f'loss_epoch{i+1}', epoch_loss)\n",
    "        mlflow.log_metric(f'accuracy_epoch{i+1}', tr_accuracy)\n",
    "    labels, predictions, eval_loss, eval_accuracy = valid(model, testing_loader, device, id2label, label2id)\n",
    "    mlflow.log_metric('eval_loss', eval_loss)\n",
    "    mlflow.log_metric('eval_accuracy', eval_accuracy)\n",
    "    mlflow.log_metric('classification_report', classification_report([labels], [predictions]))\n",
    "    mlflow.transformers.log_model(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9928bb-6a78-4606-aeb0-5d31656845ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mسلام یه متخصص پوست و مو خانوم تو هاشمیه مشهد برام پیدا کن که این هفته وقت داشته باشه\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m sentence_out, word_level_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m(sentence, model, tokenizer, id2label, device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence_out, word_level_predictions, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = \"سلام یه متخصص پوست و مو خانوم تو هاشمیه مشهد برام پیدا کن که این هفته وقت داشته باشه\"\n",
    "sentence_out, word_level_predictions = predict(sentence, model, tokenizer, id2label, device)\n",
    "print(sentence_out, word_level_predictions, sep=\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
